
The most critical optimization we have performed is loop tiling. Throughout the lab, we have found that the cache misses while reading and writting from the array is by far the most time-consuming operations. Therefore performing loop tiling would reduces the length of column and row that the read or write operations has to traverse through in each iteration of the loop and exploit spatial locality by having the working set stored in L1 cache. 

We also change the size of the loop tiling from a square tile to having longer rows and shorter columns, this is to further take advantage of the spatial locality of the adjacent row elements and also temporal localities with shorter columns. The tile used was small for large images and large for small images. This is because the extra overhead presented by loop tiling is significant for small images but becomes negligible for large images. The exact tiles sizes we decided on was 128x8 for dimensions larger than 724 and 256x16 for dimensions 724 and smaller. The reason we chose a column tile size of 8 for large images is to reduce the conflict misses in the associative sets of the L1 cache where the elements we read on consecutive rows would be placed into the same cache set.

In addition, we have found that cache misses during writting is more costly than reading, and hence we decided to read the matrix by columns and write into the new matrix by rows as operations on rows are faster in row-major paradigms. Write cache misses are more time cosuming since a miss requires writing the dirty cache lines back to memory and loading new cache lines from memories whereas only a memory load is required for read misses. Naturally we also performed loop order swapping to make sure the writing operations are executed in the most inner loop.

Finally, we performed loop unrolling on top of our loop tiling to further optimize the performance. 
